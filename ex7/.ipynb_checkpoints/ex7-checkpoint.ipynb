{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /tmp/4/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /tmp/4/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /tmp/4/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "import os\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = nltk.corpus.rte.pairs(os.path.join(os.path.abspath(''), 'data/dev.xml'))\n",
    "\n",
    "flatten_rte = lambda tags: enumerate(nltk.chunk.util.tree2conlltags(tags))\n",
    "\n",
    "#\n",
    "# Creates a list of objects with fields fromlists of RTE tags, text tokens and pos tags:\n",
    "#   from - starting index of sequence\n",
    "#   to - ending index of a sequence\n",
    "#   text - text contents of the sequence\n",
    "#   ne - NONE if the sequence is not an named entity / valid NE category otherwise\n",
    "#\n",
    "def clean_rte(rte_tags, tokens, pos_tags):\n",
    "    flat_tags = flatten_rte(rte_tags)\n",
    "    ents = []\n",
    "    last_iob_tag = 'O'\n",
    "    # Go through the IOB tagged tokens for the sentence\n",
    "    for index, t in flat_tags:\n",
    "        # Split IOB tag by \"-\"\n",
    "        iob_tags = t[2].split('-')\n",
    "        if iob_tags[0] != 'O':\n",
    "            new_label = iob_tags[len(iob_tags)-1]\n",
    "            # Rewrite the tags to match tags used by Spacy\n",
    "            if new_label == \"ORGANIZATION\":\n",
    "                new_label = \"ORG\"\n",
    "            \n",
    "            if iob_tags[0] == 'I' and last_iob_tag == 'B':\n",
    "                # Continue last tag\n",
    "                last_span = ents[len(ents)-1]\n",
    "                ents[len(ents)-1] = {\n",
    "                    \"from\": last_span[\"from\"],\n",
    "                    \"to\": last_span[\"to\"]+1,\n",
    "                    \"ne\": new_label,\n",
    "                    \"text\": \" \".join(tokens[last_span[\"from\"]:last_span[\"to\"]+1]),\n",
    "                    \"tag\": t[1],\n",
    "                }\n",
    "            else:\n",
    "                # Begin new tag\n",
    "                ents.append({\n",
    "                    \"from\": index,\n",
    "                    \"to\": index+1,\n",
    "                    \"ne\": new_label,\n",
    "                    \"text\": tokens[index],\n",
    "                    \"tag\": t[1],\n",
    "                })\n",
    "        last_iob_tag = iob_tags[0]\n",
    "    all_ents = []\n",
    "    cur_ne = 0\n",
    "    token_index = 0\n",
    "    while token_index < len(tokens):\n",
    "        if cur_ne < len(ents):\n",
    "            if ents[cur_ne][\"from\"] == token_index:\n",
    "                all_ents.append(ents[cur_ne])\n",
    "                token_index = ents[cur_ne][\"to\"]+1\n",
    "                cur_ne = cur_ne+1\n",
    "                continue\n",
    "        all_ents.append({\n",
    "            \"from\": token_index,\n",
    "            \"to\": token_index+1,\n",
    "            \"ne\": \"NONE\",\n",
    "            \"text\": tokens[token_index],\n",
    "            \"tag\": pos_tags[token_index][1],\n",
    "        })\n",
    "        token_index = token_index+1\n",
    "    return all_ents\n",
    "            \n",
    "# Store all words to print only unique ones\n",
    "all_words = set()\n",
    "\n",
    "# Store output data\n",
    "# Each line contains the tuple of\n",
    "# - token contents\n",
    "# - POS tag\n",
    "# - synonyms\n",
    "# - hypernyms\n",
    "# - entire synsets list\n",
    "lookup_data = []\n",
    "\n",
    "# Go through all pairs in the corpus\n",
    "for i, pair in enumerate(corpus):\n",
    "    # Go through text and hyp from the pair\n",
    "    for field in ['text', 'hyp']:\n",
    "        input_text = pair.text\n",
    "        if field == 'hyp':\n",
    "            input_text = pair.hyp\n",
    "        \n",
    "        # Tokenize the text and filter out all characters\n",
    "        tokens_text = [word for word in nltk.word_tokenize(input_text) if word.isalnum()]\n",
    "        \n",
    "        # POS-tagging\n",
    "        pos_text = nltk.pos_tag(tokens_text)\n",
    "        \n",
    "        # NER-tagging\n",
    "        tags_text = nltk.ne_chunk(pos_text)\n",
    "        \n",
    "        # Cleanup the NLTK NER fromat to most usable form\n",
    "        text = clean_rte(tags_text, tokens_text, pos_text)\n",
    "        \n",
    "        # Iterate through all sequences\n",
    "        for token in text:\n",
    "            token_synonyms = []\n",
    "            token_hypernyms = []\n",
    "            # If the sequence is not a named entity\n",
    "            if token[\"ne\"] == \"NONE\":\n",
    "                # Check if the word was already printed\n",
    "                if f\"{token['text']}-{token['tag']}\" not in all_words:\n",
    "                    all_words.add(f\"{token['text']}-{token['tag']}\")\n",
    "                    # If not then check all the synsets\n",
    "                    pos = token['tag'].lower()\n",
    "                    if pos not in [\"a\", \"s\", \"r\", \"n\", \"v\"]:\n",
    "                        pos = None\n",
    "                    synsets = wn.synsets(token[\"text\"], pos=pos)\n",
    "                    for syn in synsets:\n",
    "                        for lem in syn.lemmas():\n",
    "                            # Print lemmas for all the synonyms\n",
    "                            token_synonyms.append(lem.name())\n",
    "                        for hyp in syn.hypernyms():\n",
    "                            # Print all hypernyms\n",
    "                            token_hypernyms.append(hyp.name())\n",
    "                    lookup_data.append((token['text'], token['tag'], token_synonyms, token_hypernyms, synsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print output in the requested format and save it to file\n",
    "# Format:\n",
    "#\n",
    "# <token> <pos_tag>\n",
    "# <synonyms_comma_separated> <hypernims_comma_separated>\n",
    "#\n",
    "with open('wordnet_results.txt', mode='w') as results_file:\n",
    "    for token in lookup_data:\n",
    "        results_file.write(f\"{token[0]}\\t{token[1]}\\n{','.join(token[2])}\\t{','.join(token[3])}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in lookup_data:\n",
    "    token[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
