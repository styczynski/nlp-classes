{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /tmp/4/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /tmp/4/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /tmp/4/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /tmp/4/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "import os\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "rf = lambda v: str(round(v, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = nltk.corpus.rte.pairs(os.path.join(os.path.abspath(''), 'data/dev.xml'))\n",
    "\n",
    "flatten_rte = lambda tags: enumerate(nltk.chunk.util.tree2conlltags(tags))\n",
    "\n",
    "#\n",
    "# Creates a list of objects with fields fromlists of RTE tags, text tokens and pos tags:\n",
    "#   from - starting index of sequence\n",
    "#   to - ending index of a sequence\n",
    "#   text - text contents of the sequence\n",
    "#   ne - NONE if the sequence is not an named entity / valid NE category otherwise\n",
    "#\n",
    "def clean_rte(rte_tags, tokens, pos_tags):\n",
    "    flat_tags = flatten_rte(rte_tags)\n",
    "    ents = []\n",
    "    last_iob_tag = 'O'\n",
    "    # Go through the IOB tagged tokens for the sentence\n",
    "    for index, t in flat_tags:\n",
    "        # Split IOB tag by \"-\"\n",
    "        iob_tags = t[2].split('-')\n",
    "        if iob_tags[0] != 'O':\n",
    "            new_label = iob_tags[len(iob_tags)-1]\n",
    "            # Rewrite the tags to match tags used by Spacy\n",
    "            if new_label == \"ORGANIZATION\":\n",
    "                new_label = \"ORG\"\n",
    "            \n",
    "            if iob_tags[0] == 'I' and last_iob_tag == 'B':\n",
    "                # Continue last tag\n",
    "                last_span = ents[len(ents)-1]\n",
    "                ents[len(ents)-1] = {\n",
    "                    \"from\": last_span[\"from\"],\n",
    "                    \"to\": last_span[\"to\"]+1,\n",
    "                    \"ne\": new_label,\n",
    "                    \"text\": \" \".join(tokens[last_span[\"from\"]:last_span[\"to\"]+1]),\n",
    "                    \"tag\": t[1],\n",
    "                }\n",
    "            else:\n",
    "                # Begin new tag\n",
    "                ents.append({\n",
    "                    \"from\": index,\n",
    "                    \"to\": index+1,\n",
    "                    \"ne\": new_label,\n",
    "                    \"text\": tokens[index],\n",
    "                    \"tag\": t[1],\n",
    "                })\n",
    "        last_iob_tag = iob_tags[0]\n",
    "    all_ents = []\n",
    "    cur_ne = 0\n",
    "    token_index = 0\n",
    "    while token_index < len(tokens):\n",
    "        if cur_ne < len(ents):\n",
    "            if ents[cur_ne][\"from\"] == token_index:\n",
    "                all_ents.append(ents[cur_ne])\n",
    "                token_index = ents[cur_ne][\"to\"]+1\n",
    "                cur_ne = cur_ne+1\n",
    "                continue\n",
    "        all_ents.append({\n",
    "            \"from\": token_index,\n",
    "            \"to\": token_index+1,\n",
    "            \"ne\": \"NONE\",\n",
    "            \"text\": tokens[token_index],\n",
    "            \"tag\": pos_tags[token_index][1],\n",
    "        })\n",
    "        token_index = token_index+1\n",
    "    return all_ents\n",
    "            \n",
    "# Store all words to print only unique ones\n",
    "all_words = set()\n",
    "\n",
    "# Store output data\n",
    "# Each line contains the tuple of\n",
    "# - token contents\n",
    "# - POS tag\n",
    "# - synonyms\n",
    "# - hypernyms\n",
    "# - entire synsets list\n",
    "lookup_data = []\n",
    "\n",
    "# Go through all pairs in the corpus\n",
    "for i, pair in enumerate(corpus):\n",
    "    # Go through text and hyp from the pair\n",
    "    for field in ['text', 'hyp']:\n",
    "        input_text = pair.text\n",
    "        if field == 'hyp':\n",
    "            input_text = pair.hyp\n",
    "        \n",
    "        # Tokenize the text and filter out all characters\n",
    "        tokens_text = [word for word in nltk.word_tokenize(input_text) if word.isalnum()]\n",
    "        \n",
    "        # POS-tagging\n",
    "        pos_text = nltk.pos_tag(tokens_text, tagset='universal')\n",
    "        \n",
    "        # NER-tagging\n",
    "        tags_text = nltk.ne_chunk(nltk.pos_tag(tokens_text))\n",
    "        \n",
    "        # Cleanup the NLTK NER fromat to most usable form\n",
    "        text = clean_rte(tags_text, tokens_text, pos_text)\n",
    "        \n",
    "        # Iterate through all sequences\n",
    "        for token in text:\n",
    "            token_synonyms = []\n",
    "            token_hypernyms = []\n",
    "            # If the sequence is not a named entity\n",
    "            if token[\"ne\"] == \"NONE\":\n",
    "                # Check if the word was already printed\n",
    "                if f\"{token['text']}-{token['tag']}\" not in all_words:\n",
    "                    all_words.add(f\"{token['text']}-{token['tag']}\")\n",
    "                    # If not then check all the synsets\n",
    "                    pos = None\n",
    "                    synsets = wn.synsets(token[\"text\"], pos=pos)\n",
    "                    for syn in synsets:\n",
    "                        for lem in syn.lemmas():\n",
    "                            # Print lemmas for all the synonyms\n",
    "                            token_synonyms.append(lem.name())\n",
    "                        for hyp in syn.hypernyms():\n",
    "                            # Print all hypernyms\n",
    "                            token_hypernyms.append(hyp.name())\n",
    "                    lookup_data.append((token['text'], token['tag'], token_synonyms, token_hypernyms, synsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print output in the requested format and save it to file\n",
    "# Format:\n",
    "#\n",
    "# <token> <pos_tag>\n",
    "# <synonyms_comma_separated> <hypernims_comma_separated>\n",
    "#\n",
    "with open('wordnet_results.txt', mode='w') as results_file:\n",
    "    for token in lookup_data:\n",
    "        results_file.write(f\"{token[0]}\\t{token[1]}\\n{','.join(token[2])}\\t{','.join(token[3])}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_perc = sum([1 for token in lookup_data if token[1] == 'NOUN' and len(token[4]) > 0]) / len(lookup_data) * 100\n",
    "verb_perc = sum([1 for token in lookup_data if token[1] == 'VERB' and len(token[4]) > 0]) / len(lookup_data) * 100\n",
    "adj_perc = sum([1 for token in lookup_data if token[1] == 'ADJ' and len(token[4]) > 0]) / len(lookup_data) * 100\n",
    "adv_perc = sum([1 for token in lookup_data if token[1] == 'ADV' and len(token[4]) > 0]) / len(lookup_data) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouns in Wordnet:       | 48.8022%\n",
      "Verbs in Wordnet:       | 23.0294%\n",
      "Adjectives in Wordnet:  | 12.0556%\n",
      "Adverbs in Wordnet:     | 3.7481%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nouns in Wordnet:       | {rf(noun_perc)}%\")\n",
    "print(f\"Verbs in Wordnet:       | {rf(verb_perc)}%\")\n",
    "print(f\"Adjectives in Wordnet:  | {rf(adj_perc)}%\")\n",
    "print(f\"Adverbs in Wordnet:     | {rf(adv_perc)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You probably have to update NLTK before proceeding?\n",
    "#!pip3 install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import CoreNLPParser\n",
    "import io\n",
    "import time\n",
    "\n",
    "#\n",
    "# You have to start the server first using\n",
    "#    $ bash ./start_server.sh\" command\n",
    "#\n",
    "parser = CoreNLPParser(url='http://localhost:9069')\n",
    "\n",
    "with open('wordnet_trees.txt', mode='a') as results_file:\n",
    "    # Go through all pairs in the corpus\n",
    "    for i, pair in enumerate(corpus):\n",
    "        input_text = pair.text\n",
    "        tokens_text = [word for word in nltk.word_tokenize(input_text) if word.isalnum()]\n",
    "        for tree in parser.parse(tokens_text):\n",
    "            tree.pretty_print(stream=results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
