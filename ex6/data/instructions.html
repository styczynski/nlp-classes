<html>
<head>
<title>Recognising Textual Entailment Challenge Instructions</title>
<body>
<h2>Task Definition</h2>
<p>The PASCAL Challenge introduces <i>textual
entailment </i>as a generic evaluation framework for &quot;practical&quot;
semantic inference in Natural Language Processing, Information Retrieval and
Machine Learning applications.</p>
<p><i>Textual entailment</i> <i>recognition</i> is the
task of deciding, given two text fragments, whether the meaning of one text is
entailed (i.e., it can be inferred) from another text. More concretely, textual entailment is defined as a directional relationship between
pairs of text expressions, denoted by <i>T</i> - the entailing
&quot;Text&quot;, and <i>H</i> - the entailed &quot;Hypothesis&quot;. We say
that <i>T entails H</i>, denoted by <i>T</i> &#8594; <i>H</i>, if the meaning of <i>H </i>can be
inferred from the meaning of <i>T</i>, as would typically be interpreted by
people. This somewhat informal definition is based on (and assumes) common
human understanding of language and some common background knowledge.  It is
similar in spirit to evaluation of applied tasks such as Question Answering, in
which humans need to judge whether the correct answer can be inferred from a
given retrieved text.</p>

<p> This generic task captures a range of inferences that are relevant
for multiple applications. The challenge dataset includes Text-Hypothesis pairs
that correspond to typical success and failure settings of specific
applications, as detailed below, and represent different difficulty
levels of entailment reasoning, such as lexical, syntactic, morphological and
logical.</p>

<h2>Dataset Collection and Application Settings</h2>

<p> The dataset of Text-Hypothesis pairs was collected by human
annotators. It consists of seven subsets, which correspond to typical success
and failure settings in different applications (as listed below). Within each
application setting the annotators selected both positive entailment examples
(judgment as <i>TRUE</i>), where <i>T </i>does entail <i>H</i>, as well as
negative examples (<i>FALSE</i>), where entailment does not hold (roughly
50%-50% split). Some <i>T-H </i>examples appear in the table below, please look
at the development data to see more examples.</p>

<h3>Information Retrieval (IR):</h3>
<p>Annotators generated hypotheses that may
correspond to meaningful IR queries, which express some concrete semantic relations
(typically longer and more specific than a standard keyword query, thus
representing a semantic-oriented variation within IR). The hypotheses were
selected by examining prominent sentences in news stories, and were then
submitted to a web search engine. Candidate texts (<i>T</i>) were selected from
the search engine's retrieved documents, picking both texts that do or do not
entail the hypothesis.</p>

<h3>Comparable Documents (CD):</h3>
<p>Annotators identified <i>T-H </i>pairs by
examining comparable news articles which cover a common story and identifying
&quot;aligned&quot; sentence-pairs based on some lexical overlap, but where
semantic entailment may or may not hold. (Lexical overlap is a common practice
technique for processing comparable documents, for example in applications such
as multi-document summarization or paraphrase extraction).</p>

<h3>Reading Comprehension (RC):</h3>
<p>This task corresponds to a typical reading
comprehension exercise in language teaching, were students are asked to judge
whether a particular assertion may be inferred from a given text story. The
challenge annotators were asked to create such hypotheses relative to text
sentences in news documents. The annotators where instructed to consider a
reading comprehension test for high school students. </p>

<h3>Question Answering (QA):</h3>
<p>Using a newspaper-based corpus built for QA
experiments, annotators selected some questions and turned them into
affirmative sentences with the correct answer &quot;plugged in&quot;. These
affirmative sentences serve as the hypotheses (H). Then the annotators chose
relevant text snippets (T) that are suspected to indicate the correct answer,
producing entailment pairs. For example, given the question, &quot;<i>Who is Ariel
Sharon</i>?&quot; and an expected answer text &quot;<i>Israel's prime Minister, Ariel
Sharon, visited Prague</i>&quot; (T)<i>,</i> the question is turned it into the
statement <i>&quot;Ariel Sharon is the Israeli Prime Minister&quot; (H)</i>, producing a <i>TRUE</i>
entailment pair.</p>

<h3>Information Extraction (IE):</h3>
<p>Comment: This task is inspired by the
Information Extraction application, adapting the setting for having pairs of
texts rather than a text and a structured template.</p>

<p>Given a set of IE relations of interest
(e.g. a management succession event), annotators identified as the text (T)
candidate news story sentences in which the relation might (or might not) hold.
As a hypothesis they created a common natural language formulation of the IE
relation, which is assumed to be very easy to identify by an IE system. For
example, given the text &quot;<i>Guerrillas killed a peasant in the city of Flores.</i>&quot;
(T), and the information extraction task of identifying killed civilians, a
hypothesis &quot;<i>Guerrillas killed a civilian</i>&quot; is created producing a <i>TRUE</i>
entailment pair.</p>

<h3>Machine Translation (MT): </h3>

<p>Two translations of the same text, an
automatic translation and a gold standard human translation, were compared and
modified in order to obtain T-H pairs, where correct translation correspond to <i>TRUE
</i>entailment. Automatic translations were sometimes grammatically adjusted,
being otherwise grammatically unacceptable.</p>

<h3>Paraphrase Acquisition (PP)</h3>
<p>Similar meanings can be expressed in quite
different ways, where not only the lexis varies but also the syntactical
structure of expressions. Paraphrase acquisition systems attempt to acquire
pairs (or sets) of expressions that paraphrase each other. Annotators exploited
candidate pairs of paraphrase expressions produced by an automatic paraphrase
acquisition system. They collected pairs of similar <i>T-H </i>sentences in
which one sentence contains one expression and the other contains its
paraphrase.  </p>

<div align=center>
<table border=1 cellspacing=1 cellpadding=5 summary="T-H example pairs">
<caption>Table 1: Example <i>T-H </i>pairs</caption>
 <tr> <th style="TEXT-ALIGN: center">ID</th>  
      <th style="TEXT-ALIGN: center">TEXT</th> 
      <th style="TEXT-ALIGN: center">HYPOTHESIS</th> 
      <th style="TEXT-ALIGN: center">TASK</th>
      <th style="TEXT-ALIGN: center">ENTAILMENT</th> 
</tr>
 <tr style='height:34.2pt'>
  <td>1</td>
  <td><i>iTunes software has seen strong sales in Europe.</i></td>
  <td><i>Strong sales for iTunes in Europe.</i></td>
  <td>IR</td>
  <td>TRUE</td>
 </tr>
 <tr>
  <td>2</td>
  <td><i>Cavern Club sessions paid the Beatles &#163;15 evenings and &#163;5 lunchtime.</i></td>
  <td><i>The Beatles perform at Cavern Club at lunchtime.</i></td>
  <td>IR</td>
  <td >TRUE</td>
 </tr>
 <tr>
  <td>3</td>
  <td><i>American Airlines began laying off hundreds of
  flight attendants on Tuesday, after a federal judge turned aside a union's
  bid to block the job losses.</i></td>
  <td><i>American Airlines will recall hundreds of flight
  attendants as it steps up the number of flights it operates.</i></td>
  <td>PP</td>
  <td>FALSE</td>
 </tr>
 <tr>
  <td>4</td>
  <td><i>The two suspects belong to the 30th Street gang,
  which became embroiled in one of the most notorious recent crimes in Mexico:
  a shootout at the Guadalajara airport in May, 1993, that killed Cardinal Juan
  Jesus Posadas Ocampo and six others.</i></td>
  <td><i>Cardinal Juan Jesus Posadas Ocampo died in 1993.</i></td>
  <td>QA</td>
  <td>TRUE</td>
 </tr>
</table>

</div>

<p>Annotators were instructed to replace
anaphors with the appropriate reference from preceding sentences where
applicable, and to possibly shorten the sentences in order to keep the texts
and hypotheses relatively short. All example <i>T-H </i>pairs were first judged
(as <i>TRUE/FALSE</i>) by the annotator that created the example. The examples
were then cross-evaluated by a second judge, who received only the text and
hypothesis without the original additional context. Pairs for which there was
disagreement among the judges were discarded, while the agreed decisions for
the rest of the examples are considered as the gold standard for evaluation.  </p>

<p>Some additional judgment criteria and guidelines are listed below (examples are
taken from Table 1):</p>
<ul>
<li>Given
that the text and hypothesis might originate from documents in different points
in time, tense issues are ignored. For instance, example #2 is considered true
in spite of the tense differences.  </li>

<li>Entailment is a directional relation. The hypothesis must be entailed from the given text,
but the text need not be entailed from the hypothesis. </li>

<li>The hypothesis must be fully entailed by the text.  Judgment would be <i>FALSE </i>if
the hypothesis includes parts that cannot be inferred from the text. </li>

<li>Cases in which inference is very probable (but not completely certain) are judged at <i>TRUE</i>. In example #4 one could claim that the shooting took place in 1993 and that
(theoretically) the cardinal could have been just severely wounded in the
shooting and has consequently died a few months later in 1994. However, this
example is tagged a true since it is reasonable to believe that he actually
died in 1993. On the other hand, annotators were guided to avoid vague examples
for which inference has some positive probability which is not clearly very
high. </li>
</ul>

<h2>Data Sets and Format</h2>

<p>Both Development and Test sets are formatted as XML files. 
The template will be as follows:</p>
<pre>
&lt;pair id=&quot;id_num&quot; task=&quot;task_acronym&quot; value=&quot;TRUE|FALSE&quot;&gt;

   &lt;t&gt; the text... &lt;/t&gt;

   &lt;h&gt; the hypothesis... &lt;/h&gt;

&lt;/pair&gt;
</pre>


<p>Where:</p>
<ul>
<li>each <i>T-H</i> pair appears within a single &lt;pair&gt; element.
<li>the element &lt;pair&gt; has the following attributes:
<ul>
<li>id, a unique numeral identifier of the <i>T-H</i> pair 
<li>task, the acronym of the application setting from which the pair has been generated,
according to the list above.
<li>value (in the development set only), the gold standard entailment annotation, being either 'TRUE' or 'FALSE'
</ul>
<li>the element &lt;t&gt; (text) has no attributes, and it may be made up of one or more sentences.
<li>the element &lt;h&gt; (hypothesis) has no attributes, and it usually contains a simple sentence.
</ul>


<p>The data is split to a development set and a test
set, to be released separately. The goal of the development set is to guide the
development and tuning of participating systems. Notice that since the given
task has an unsupervised nature it is not expected that
the development set can be used as a main resource for supervised training,
given its anecdotal coverage. Rather it is assumed that
systems will be using generic techniques and resources that are suitable for
the news domain.</p>

<h2>Submission</h2>

<p>Systems should tag 
each <i>T-H </i>pair as either TRUE, predicting that entailment does hold for
the pair, or as FALSE otherwise. Results will be submitted in a file with one
line for each <i>T-H </i>pair in the test set, in the following format:</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;pair_id&lt;blank space&gt;judgment&lt;blank space&gt;confidence_score</p>


<p>where:</p>
<ul>
<li>pair_id is the unique identifier of each T-H pair, in the same order as it
appears in the test set;</li>
<li>judgment is either TRUE or FALSE.</li>
<li>Confidence_score is an optional integer or floating point value
(maximum length is 8 characters) that can range between 0 and 1, inclusive,
where 0 means that the system has no confidence of the correctness of its
judgment, and 1 means that the system is absolutely confident about the
correctness of its judgment. The confidence values should be normalized to the
[0,1] range. If a system does not produce any confidence score then all pairs
will be regarded as having a confidence score of 1.</li>
</ul>

<p>The first lines of a run may look like this:</p>
<pre>
1 TRUE 0.348
2 FALSE 0.221
3 FALSE 0.873
4 TRUE 1
5 FALSE 0.003
</pre>

<p>Participating teams will be allowed to submit results of up to 2  systems. The corresponding result files should be named run1.txt (and run2.txt for a second submitted run). </p>
<p>The results files should be zipped and submitted via the <a href="http://www.pascal-network.org/Challenges/RTE/Submit/">submit form</a>.</p>

<p>Systems should be developed based on the development data set. We regard
it as acceptable to run automatic knowledge acquisition methods (such as
synonym collection) specifically for the lexical and syntactic constructs that
will be present in the test set, as long as the methodology and procedures are
general and not tuned specifically for the test data. If absolutely necessary,
we ask that participants report any other changes they perform in their systems
after downloading the test set. </p>

<h3>Partial Coverage Submissions</h3>
<p>In order to encourage systems and methods which do not cover all phenomena present in the test examples we allow submission of partial coverage results, for only part of the test examples. Any run which will not include judgments for all test examples will be considered as a partial submission and will be evaluated accordingly for its coverage (see next section). Naturally, the decision as to on which examples the system abstains should be done automatically by the system (with no manual involvement). We ask that participants that provide partial coverage results will include in their report some description and analysis of the types of examples that their system covers, versus the types of examples that are not addressed.</p>


<h2>Evaluation Measures</h2>

<p>The evaluation of all submitted runs will be automatic. The judgments
(classifications) returned by the system will be compared to those manually
assigned by the human annotators (the Gold Standard). The percentage of
matching judgments will provide the accuracy of the run, i.e. the fraction of
correct responses.</p>

<p>As a second measure, a <i>Confidence-Weighted Score</i>
(CWS, also known as Average Precision) will be computed. Judgments of the test
examples will be sorted by their confidence (in decreasing order from the most
certain to the least certain), calculating the following measure:</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;1/n * sum for i=1 to n (#-correct-up-to-pair-i/i)</p>

<p>where <i>n</i> is the number of the pairs in the test
set, and <i>i</i> ranges over the pairs.</p>

<p>The <i>Confidence-Weighted Score</i> ranges between 0 (no correct judgments at all) and 1
(perfect score), and rewards the systems' ability to assign a higher confidence
score to the correct judgments than to the wrong ones. This score will not be
computed for systems not supplying confidence values.</p>

<h3>Partial Coverage Submission Evaluation</h3>
<p>Partial coverage submissions will be evaluated separately. Accuracy and CWS (as described above), as well as coverage (number of examples in run / total number of examples), will be reported for each partial run. To visualize the relative performance of different systems we will plot the position of the various runs on accuracy/coverage and average precision/coverage graphs.</p>

<h2>Final Notes</h2>

<p>The goal
of the challenge is to provide a first opportunity for presenting and comparing
possible approaches for textual entailment recognition, aiming at an
explorative rather than a competitive setting. Therefore, even though system
results will be reported, there will not be an official ranking of systems. </p>

<p>The challenge seems to be difficult, and obtaining
relatively low results will not be surprising.</p>

<p>Never the less, it provides a benchmark for a novel
task and will supply meaningful baselines and analyses for the performance of
current systems. It is also encouraged to submit results of simple baseline
techniques, either as a second or a sole run, in order to provide additional
insights into the problem.</p>

<p>The setting of this challenge is somewhat biased, as
we specifically chose non-trivial pairs for which some inference is needed and
also imposed a balance of <i>TRUE </i>and <i>FALSE </i>examples. For this reason,
system performance in applicative settings might be higher than the figures for
the challenge data, due to a more favourable distribution of examples in real
applications.</p>

<p>Finally, the task definition and evaluation
methodologies are clearly not mature yet. We expect them to change over time
and hope that participants' contributions, observations and comments will help
shaping this evolving research direction.</p>

<h2>Acknowledgements</h2>
<p>The following sources were used in the preparation of the data:</p>
<ul>
<li>Document Understanding Conferences (DUC) 2004 <a href="http://duc.nist.gov/duc2004/">Machine Translation evaluation data</a>, from the National Institute of Standards and Technology (NIST). 
<li>TextMap Question Answering <a href="http://brahms.isi.edu:8080/textmap/">online demo</a>, from the Information Sciences Institute (ISI) 
<li><a href="http://l2r.cs.uiuc.edu/~cogcomp/">Relation Recognition dataset</a>, from University of Illinois at Urbana-Champaign. 
<li>DIRT paraphrase database (<a href="http://www.isi.edu/~pantel/Content/Demos/demosDirt.htm">online demo</a>), from the University of southern California.
<li><a href="http://www1.cs.columbia.edu/~noemie/alignment/">Corpus of Sentence Alignment in monolingual comparable corpora</a>, Columbia University.
</ul>
<p>We would like to thank the people and organizations that made these sources available for the challenge.</p>

<p>We'd also like to acknowledge the people involved in creating and annotating the data: Danilo Giampiccolo, Tracy Kelly, Einat Barnoy, Allesandro Valin, Ruthie Mandel, and Melanie Joseph.
</p>
</body>
</html>